{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algerbra Part 1  \n",
    "At first we are going to cover basics of linear algebra, which is a great way to begin because of its importance.\n",
    "\n",
    "In mathematics Linear algebra is the study of vectors and certain rules to manipulate vectors. It plays crucial role in Machine Learning and also it is said that Linear algebra lies at the heart of DeepLearning. Having a detailed explanation of theory and implementing the concepts learnt with some examples, we will understand and gain knowledge on how does Linear algebra helps the complex models solve equations in multidimensional space and to find the underlying patterns of data.  \n",
    "\n",
    "**At the end of this topic you will be able to :**\n",
    "- Understand the basics of Linear algebra.\n",
    "- Learn how to use numpy library to implement the operations in vector and matrix algebra\n",
    "- Know how the underlying equations in machine learning models work.\n",
    "\n",
    "**Resources :**  \n",
    "- ðŸ“• Given already the link to Mathematics for ML book in the main page refer it for better insights.\n",
    "- ðŸŽ¬[\"M4ML- Linear algebra by ImperialCollege\"](https://www.youtube.com/playlist?list=PLiiljHvN6z1_o1ztXTKWPrShrMrBLo5P3)  this YouTube playlist also helps a lot for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics we cover here..\n",
    "1. [Vectors, scalars operations](#vectors-and-scalars)\n",
    "2. [Projection](#projection)\n",
    "3. [Orthogonal and Orthonormal](#orthogonal-and-orthonormal-vectors)\n",
    "4. [Matrices properties and operations](#matrices-and-operations)\n",
    "5. [System of Linear equations](#system-of-linear-equations)   \n",
    "6. [Norms](#norms)\n",
    "7. [Eigen values and Eigen vectors](#eigen-values-and-eigen-vectors) \n",
    "9. [Vector spaces and sub spaces](#vector-spaces-and-sub-spaces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors and Scalars\n",
    "Vector : Vectors are used to represent quantities that have both a size (magnitude) and a direction, such as displacement, velocity, force, and many other physical quantities. vectors with magnitude and direction can be used to represent various types of data, features, and parameters.\n",
    " \n",
    "Scaler : A scalar is a single numerical value that represents a quantity with magnitude only (no direction).\n",
    "\n",
    "Looking into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representation and Operations**  \n",
    "Vector is actually created using ***'numpy.array()'*** method which takes a list of scalars as argument and return a ***'nump.ndarray'*** vector object.  \n",
    "We can perform Addition, Subraction, Multiplication of vectors with a scalar and with an another vector. We cannot divide a vector with an another vector but we can divide it with a scalar.  \n",
    "Addition, subraction, division and Multiplication between vectors must need same size of vectors. \n",
    "Division of a vector by another vector not possible at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy the library for performaing operations among vectors and matrices\n",
    "# we use import keyword to 'import' any library or package and 'as' to name an alias to the original imported package \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# vector is basically a list or array but we use numpy to represent those arrays we can also represent it by lists.\n",
    "# the reason is that numpy is written in C and C++ which makes it much faster while operating large vectors or datasets  \n",
    "\n",
    "vector_1 = np.array([10, 11, 12, 13, 14]) # this is how you define an array in numpy called 'numpy array'\n",
    "vector_2 = np.array([1, 2, 4, 5, 8])\n",
    "\n",
    "print(type(vector_1)) # the type of vector_1 is 'numpy.ndarray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition : This operation increases the magnitude of the vector while maintaining its direction.  \n",
    "Subraction : This operation decreases the magnitude of the vector while maintaining its direction.  \n",
    "Multiplication : Scaling the vector, which affects its magnitude and, potentially, its direction.\n",
    "- If c is positive, the direction of the vector remains the same.\n",
    "- If c is negative, the direction of the vector is reversed.  \n",
    "\n",
    "Dividing : This is not a standard operationin algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 13 16 18 22]\n"
     ]
    }
   ],
   "source": [
    "# adding two vectors simply gives the sum of each corresponding elements in the vectors.\n",
    "\n",
    "vector_sum = vector_1 + vector_2\n",
    "print(vector_sum) # gives the sum of individual elements in the vectors\n",
    "# subracting does the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar multipled:  [ 2  4  8 10 16]\n",
      "divided by scalar:  [0.1 0.2 0.4 0.5 0.8]\n",
      "scalar divided by vector:  [10.    5.    2.5   2.    1.25]\n"
     ]
    }
   ],
   "source": [
    "# multiplying the vectors with a scalar and Dividing vector with a scalar.\n",
    "# when vector is multiplied by a scalar or vice versa it yields new vector of same shape with all the elements multiplied with the scalar\n",
    "# when vector divided by a scalar it yields new vector of same shape with all the elements divided by scalar.\n",
    "# when scalar divided by a vector it yields new vector of same shape with scalar is divided by all the corresponding elements of vector.\n",
    "\n",
    "\n",
    "vector_mul_scalar = vector_2 * 2 # same with '2 * vector_2' doubles the magnitude\n",
    "print(\"scalar multipled: \" ,vector_mul_scalar) # result vector all elements are multiplied by the scalar 2\n",
    "\n",
    "vector_div = vector_2 / 10 # all elements divided by the scalar\n",
    "print(\"divided by scalar: \", vector_div)\n",
    "\n",
    "vector_div = 10 / vector_2 # scalar 10 is divided by all the corresponding elements\n",
    "print(\"scalar divided by vector: \",vector_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot Product :**  \n",
    "- The dot product takes two equal-lengthed vectors, multiply corresponding elements and add the products then returns a single scalar.  \n",
    "- Dot product between two vectors result in a scalar value.\n",
    "- Geometrically it is product of magnitude of a, b, and cosine of angle between them **A . B = |A| * |B| \\* cos(Î¸)**.\n",
    "- It is used in vector projection to find the component of one vector in the direction of another.\n",
    "- dot product annotaed like **A . B, A<sup>T</sup>B**\n",
    "\n",
    "**Cross Product :**  \n",
    "- The cross product is a binary operation on two vectors in 3D space. Cross product can only be done on 3D vectors and not Matrices.  \n",
    "- In three-dimensional space, vectors can be used to represent directed line segments, and the cross product provides a way to calculate a vector that is perpendicular to the plane containing the original vectors.  \n",
    "- Geometrically it is product of magnitude of a, b, and cosine of angle between them the resultant scalar multiplied by a unit vector in the direction perpendicular to both A and B **A x B = (|A| * |B| \\* sin(Î¸)) \\* n**. \n",
    "\n",
    "You can find the implementation of the cross product in below image.\n",
    "<center><img src=\"images/CrossProduct.png\" width=\"350\" height=\"220\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vector multiplilcation*** is done by either dot product or cross product based on our use. We can use ***np.dot()*** or ***np.multiply()*** or ***np.matmul()***.\n",
    "-  dot products are between vectors only.\n",
    "- np.dot() and np.matmul() perform necessary multiplication but np.multiply() just multiply the corresponding elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11 12 13 14]   [1 2 4 5 8]\n",
      "Dot Product : 257\n",
      "matmul : 257\n",
      "multiply: [ 10  22  48  65 112]\n"
     ]
    }
   ],
   "source": [
    "# Using np.dot() \n",
    "result_dot_vector = np.dot(vector_1, vector_2)\n",
    "# Using np.matmul() \n",
    "result_matmul_vector = np.matmul(vector_1, vector_2)\n",
    "# Using np.multiply() \n",
    "result_multiply_vector = np.multiply(vector_1, vector_2)\n",
    "\n",
    "print(vector_1, \" \" ,vector_2)\n",
    "print(\"Dot Product :\",result_dot_vector)\n",
    "print(\"matmul :\",result_matmul_vector)\n",
    "print(\"multiply:\",result_multiply_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "The projection of a vector onto another vector is a fundamental operation that represents the \"shadow\" of one vector onto the other.  \n",
    "The projection of vector A onto vector B is the vector component of A that lies in the direction of B.  \n",
    "\n",
    "Below shown the formula for projection of vector A onto vector v.\n",
    "<center><img src=\"images/Projection.png\" width=\"300\" height=\"100\" align=\"center\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal and Orthonormal vectors\n",
    "\n",
    "***Orthogonal vectors*** :   \n",
    "Two vectors are orthogonal if their dot product is zero. Mathematically, vectors A and B are orthogonal if  **A â‹… B = 0**. Geometrically, orthogonal vectors are perpendicular to each other in n-dimensional space.  \n",
    "Orthogonal vectors do not have any component in the direction of each other. They form a right angle, and the cosine of the angle between them is zero. \n",
    "\n",
    "***Orthonormal vectors*** :  \n",
    "A set of vectors is orthonormal if all vectors are orthogonal to each other and have unit length. let **A<sub>1</sub>, A<sub>2</sub> . . . A<sub>n</sub>** be set of Vectors are orthonormal if :  \n",
    "**A<sub>i</sub> . A<sub>j</sub> = 0,   i != j**  \n",
    "**A<sub>i</sub> = 1,  for all i**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices and Operations\n",
    "\n",
    "Matrix : An ordered set of numbers arranged in two dimensions, forming rows and columns. Each element in a matrix is identified by two indices, one for the row and one for the column. A 2D matrix indeed contains multiple 1D vectors, where each row or column can be considered a 1D vector.   \n",
    "In ML we often represent datasets(data) in the form of matrices, and many ML algorithms involved in computation of Matrix operations. More advanced concepts in matrices also has specific uses in the advanced ML algorithms which will be covered further. \n",
    "\n",
    "Matrix operations like addition and subtraction are similar to the vectors where adding corresponding elements in both matrices. Similarly there exists no matrix to matrix division. And matrix to matrix multiplication is done using cross product which yields a new matrix. we can do it using both ***np.dot()*** and ***np.matmul()***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dot: \n",
      " [[10 13  9]\n",
      " [ 5  7  6]\n",
      " [12 15 10]]\n",
      "usng matmul: \n",
      " [[10 13  9]\n",
      " [ 5  7  6]\n",
      " [12 15 10]]\n"
     ]
    }
   ],
   "source": [
    "matrix_A = np.array([[1, 2, 3], [1, 1, 1], [1, 2, 4]])\n",
    "matrix_B = np.array([[2, 3, 4], [1, 2, 1], [2, 2, 1]])\n",
    "\n",
    "matrix_dot = np.dot(matrix_A, matrix_B)\n",
    "matrix_mul = np.matmul(matrix_A, matrix_B)\n",
    "\n",
    "print(\"using dot: \\n\", matrix_dot)\n",
    "print(\"usng matmul: \\n\",matrix_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose of a matrix\n",
    "The transpose of a matrix is found by interchanging its rows into columns or columns into rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [5 6 7]]\n",
      "Transpose: \n",
      " [[1 3 5]\n",
      " [2 4 6]\n",
      " [3 5 7]]\n"
     ]
    }
   ],
   "source": [
    "T = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n",
    "print(T)\n",
    "print(\"Transpose: \\n\",T.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Matrix\n",
    "Square Matrix having only diagonal elements value of 1 all other 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "I = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Matrix\n",
    "Matrix which is equal to its Transpose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 6]\n",
      " [3 4 5]\n",
      " [6 5 9]]\n",
      "Symmetric Matrix: \n",
      " [[2 3 6]\n",
      " [3 4 5]\n",
      " [6 5 9]]\n"
     ]
    }
   ],
   "source": [
    "S = np.array([[2, 3, 6], [3, 4, 5], [6, 5, 9]])\n",
    "print(S)\n",
    "print(\"Symmetric Matrix: \\n\",S.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace of a matrix \n",
    "The sum of the diagonal elements in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 6]\n",
      " [3 4 5]\n",
      " [6 5 9]]\n",
      "Trace of Matrix:  15\n"
     ]
    }
   ],
   "source": [
    "S = np.array([[2, 3, 6], [3, 4, 5], [6, 5, 9]])\n",
    "print(S)\n",
    "print(\"Trace of Matrix: \",S.trace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinant of a matrix\n",
    "\n",
    "The determinant of a square matrix is a scalar value that can be computed from its elements. The determinant is a measure of the \"scaling factor\" of the linear transformation represented by the matrix.  \n",
    "Determinant only exists for a square matrix and also inverse .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determinant of S:  12.0\n"
     ]
    }
   ],
   "source": [
    "S = np.array([[2, 3, 3], [3, 4, 5], [6, 5, 1]])\n",
    "print(\"determinant of S: \",np.linalg.det(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse of a Matrix\n",
    "\n",
    "- We denote its inverse as **A<sup>-1</sup>**. The inverse of a matrix is another matrix that, when multiplied by the given matrix, yields the multiplicative identity.   \n",
    "- For a matrix **A**, its inverse is **A<sup>-1</sup>**. And **A . A<sup>-1</sup> = I**, where **I** is denoted as the identity matrix.  \n",
    "- Same as determinant Inverse of a matrix only exists for a square matrix and a non singular matrix, which means a matrix having a non zero determinant.  \n",
    "- The inverse of a matrix is the adjoint matrix of **A** multiplied by inverse of determinant of **A**.  \n",
    "- The adjoint of a matrix, also called the adjugate of a matrix, is defined as the transpose of the cofactor matrix of that particular matrix. For a matrix A, the adjoint is denoted as adj (A).  \n",
    "- Inverse of matrix has some important useful in complex calculations. For example in Linear regression.\n",
    "\n",
    "Formula for Inverse of a matrix is denoted below:\n",
    "\n",
    "<center><img src=\"images/inverse.webp\" width=\"300\" height=\"100\" align=\"center\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of S:\n",
      " [[ 0.  0.  1.]\n",
      " [-2.  1.  3.]\n",
      " [ 3. -1. -5.]]\n",
      "result Identity matrix:\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "S = np.array([[2, 1, 1], [1, 3, 2], [1, 0, 0]])\n",
    "inv_S = np.linalg.inv(S)\n",
    "\n",
    "print(\"Inverse of S:\\n\",inv_S)\n",
    "\n",
    "result = np.dot(S, inv_S)\n",
    "print(\"result Identity matrix:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of a Matrix  \n",
    "The maximum number of linearly independent columns (or rows) of a matrix is called the rank of a matrix.  \n",
    "The rank of a matrix cannot exceed the number of its rows or columns.  \n",
    "If we consider a square matrix, the columns (rows) are linearly independent only if the matrix is nonsingular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of matrix:  3\n"
     ]
    }
   ],
   "source": [
    "R = np.array([[2, 1, 1], [1, 3, 2], [1, 0, 0]])\n",
    "print(\"Rank of matrix: \",np.linalg.matrix_rank(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System of Linear Equations\n",
    "- System of linear equations are group of linear equations can be represented in matrix form using a coefficient matrix, a variable matrix, and a constant matrix.   \n",
    "- They will be represented using an equation **A x = b** where A is the linear equation(coefficient) matrix, x is variable matrix and b is constant matrix.  \n",
    "\n",
    "**Solving the linear equations**:   \n",
    "- We can solve the linear equations by elimination or substitution methods. We can explore the method of using rank.\n",
    "- Given the linear system **Ax = B** and the augmented matrix **(A|B)**.  \n",
    "- Augmented matrix is just adding the constant matrix **b** as the column vector to matrix **A**.   \n",
    "1. If rank(A) = rank(A|B) = the number of rows in x(variables), then the system has a unique solution.   \n",
    "2. If rank(A) = rank(A|B) < the number of rows in x(variables), then the system has infintely many solutions.  \n",
    "3. If rank(A) < rank(A|B), then the system is inconsistent.\n",
    "\n",
    "\n",
    "Below shown how actually the two linear equations will be represented as per the formula..  \n",
    "**2 - x = 0**  \n",
    "**-x + y = 3**\n",
    "<center><img src=\"images/SystemOfEq.webp\" width=\"300\" height=\"170\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of A: 2\n",
      "Rank of [A|B]: 2\n",
      "Solution for Ax=B:  [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient matrix\n",
    "A = np.array([[2, -1], [-1, 2]])\n",
    "\n",
    "# Right-hand side vector\n",
    "B = np.array([0, 3])\n",
    "\n",
    "# Augmented matrix [A|B]\n",
    "AB = np.column_stack((A, B))\n",
    "\n",
    "# Find the ranks\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "rank_AB = np.linalg.matrix_rank(AB)\n",
    "\n",
    "#satisfies the condition of  rank(A) = rank(A|B) = the number of rows in x(variables) hence has unique solution\n",
    "print(\"Rank of A:\", rank_A)\n",
    "print(\"Rank of [A|B]:\", rank_AB)\n",
    "\n",
    "#solution\n",
    "solution = np.linalg.solve(A,B) \n",
    "# we use np.linalg.solve() method to solve the linear equations \n",
    "# but it throws error when the equations have either no solution or infinitely many solution.\n",
    "\n",
    "print(\"Solution for Ax=B: \", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "- In mathematics, a norm is a function that assigns a positive real number to each vector in a vector space.   \n",
    "- Norms are used to measure the size or length of vectors and are a generalization of the concept of absolute value in one-dimensional spaces\n",
    "- Mainly discuss about two Norms here L1(Manhattan / Taxicab) Norm, L2(Euclidean) Norm.\n",
    "\n",
    "**L1 or Manhattan norm**:  \n",
    "- Manhattan distance the sum of the absolute differences between the coordinates.\n",
    "- **distance = |x1 - x2| + |y1 - y2|**\n",
    "\n",
    "**L2 or Euclidean norm**:  \n",
    "- Euclidean distance the square root of sum of squared differnces of coordinates.\n",
    "- Its the basic distance formula between two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan Distance: 7.0\n",
      "Euclidean Distance: 5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Manhattan Distance (L1-norm)\n",
    "P = np.array([2, 3])\n",
    "Q = np.array([5, 7])\n",
    "\n",
    "# why Q - P is its  the basic operation of finding the distance between two points\n",
    "# the expression Qâˆ’P represents the vector pointing from point P to point Q. \n",
    "# The resulting vector provides the direction and magnitude needed to go from P to Q.\n",
    "\n",
    "manhattan_distance = np.linalg.norm(Q - P, ord=1)\n",
    "\n",
    "# Euclidean Distance (L2-norm)\n",
    "euclidean_distance = np.linalg.norm(Q - P, ord=2)\n",
    "\n",
    "print(\"Manhattan Distance:\", manhattan_distance)\n",
    "print(\"Euclidean Distance:\", euclidean_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen values and Eigen vectors\n",
    "\n",
    "- Linear transformations of a vector is scaling, rotating and shearing of the vector and transforming it into a new form.\n",
    "- They are mathematical operations that alter the shape, orientation, or position of vectors while preserving certain properties \n",
    "<br></br>\n",
    "- Eigen vectors are those vectors which lie along the same span in both before and after a Linear transformation to a space.\n",
    "- Eigen values are the amount of each of those eigen vectors has streched or shrinked.\n",
    "- Eigen vectors and eigen values are most useful in PCA, Image and Signal processings.\n",
    "<br></br>\n",
    "- For a square matrix **A**, a scalar **Î»** is considered an eigenvalue if there exists a non-zero vector **v** (eigenvector) such that **Av = Î»v**.\n",
    "-  A non-zero vector **v** is an eigenvector of a square matrix **A** if **Av = Î»v**, where **Î»** is the corresponding eigenvalue.\n",
    "\n",
    "Below shown the formula : \n",
    "<center><img src=\"images/EigenValues_and_Vectors.png\" width=\"300\" height=\"220\" align=\"center\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 1  2  1]\n",
      " [ 6 -1  0]\n",
      " [-1 -2 -1]]\n",
      "\n",
      "Eigenvalues:\n",
      "[-4.00000000e+00  3.00000000e+00  1.59494787e-16]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.40824829 -0.48507125 -0.0696733 ]\n",
      " [-0.81649658 -0.72760688 -0.41803981]\n",
      " [-0.40824829  0.48507125  0.90575292]]\n"
     ]
    }
   ],
   "source": [
    "# 3x3 matrix\n",
    "A = np.array([[1, 2, 1],\n",
    "              [6, -1, 0],\n",
    "              [-1, -2, -1]])\n",
    "\n",
    "# Find eigenvalues and eigenvectors\n",
    "# function 'numpy.linalg.eig()' will return the eigenvalues and eigenvectors of the matrix A.\n",
    "eigenvalues_B, eigenvectors_B = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues_B)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Spaces and Sub Spaces\n",
    "\n",
    "- If **n** is a positive integer, then an ordered n-tuple is a sequence of **n** real numbers (**a<sub>1</sub>, a<sub>2</sub>, . . . , a<sub>n</sub>)**.  \n",
    "- The set of all ordered n-tuples is called n-space and is denoted by R<sup>n</sup>.\n",
    "<br></br>\n",
    "- If **n = 1** then all the tuples have exactly one real number then **R** will be the set of real numbers.\n",
    "- The vector space **R<sup>3</sup>**, is the set of ordered triples, which describe all points and directed line segments in 3D space.\n",
    "- You can term those triples as 3D point or a vector.\n",
    "<br></br>\n",
    "- A vector space **V** over a field **F** is a nonempty set on which two operations are defined - addition and scalar multiplication.\n",
    "- Say objects **u** and **v** in space **V** and scalar **k** belongs **F** . They should satisfy the following axioms.\n",
    "\n",
    "1. If **u**, **v âˆˆ V** , then **u + v âˆˆ V** .\n",
    "2. If **u âˆˆ V** and **k âˆˆ F**, then **ku âˆˆ V** .\n",
    "3. **u + v = v + u**\n",
    "4. **u + (v + w) = (u + v) + w**\n",
    "5. There is an object 0 in **V**, called a zero vector for **V** , such that **u + 0 = 0 + u = u** for all **u** in **V** .\n",
    "6. For each **u** in **V** , there is an object **-u** in **V** , called the additive inverse of **u**,such that **u + (âˆ’u) = âˆ’u + u = 0**\n",
    "7. And associative rule, commutative rule.\n",
    "<br></br>\n",
    "- A subset **W** of a vector space **V** is called a subspace of **V** if **W** is itself a vector space under the addition and scalar multiplication defined on **V**.\n",
    "- That is simply satisfying all the axioms of Vector space.\n",
    "\n",
    "#### I suggest you to explore **Eigen Vectors, Eigen Values, Vector Spaces and Subspaces** in-depth through additional resources for a comprehensive understanding beyond concise explanations. Because these axioms and statements might not give you a perfect explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr>\n",
    "\n",
    "- #### In the next chapter or module we will look into SVD, EVD, Gradients, Tensors etc..,\n",
    "#### Like I said refer to some text book whenever you get stuck. \n",
    "#### I hope this was helpful, I've given you a high-level understanding of what linear algebra is and how we implement it using python numpy library. I think this level is enough for you to continue your progress. \n",
    "\n",
    "- All kinds of suggestions are acceptable please feel free to reach out to me my **LinkedIn [Govardhan V](https://www.linkedin.com/in/govardhan-vembadi/)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
