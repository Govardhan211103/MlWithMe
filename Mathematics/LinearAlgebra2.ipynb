{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algerbra Part 2\n",
    "Looking further into Linear Algebra we got left with the following topics and we will cover them here in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Matrix decomposition\n",
    "2. SVD\n",
    "3. Diagonalization\n",
    "4. EVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Decompostion\n",
    "Matrix Decomposition or Matrix Factorization, is a fundamental concept in linear algebra.\n",
    "- Matrix decomposition is a way of reducing a matrix into its fundamental components(simpler decomposed matrices).\n",
    "- It simplifies complex matrix operations by allowing us to perform calculations on the decomposed matrix rather than the original matrix itself.\n",
    "- Many machine learning algorithms involve matrix operations, such as solving linear systems of equations, calculating inverses, and determining determinants.\n",
    "- Matrix decomposition provides a foundation for these operations, making them more efficient and numerically stable.\n",
    "\n",
    "**Matrix Decompostion Techniques**:\n",
    "- *LU (Lower-Upper) Decomposition*: Decomposes a square matrix into lower and upper triangular matrices.\n",
    "- *QR Decomposition*: Factorizes a matrix into an orthogonal matrix (Q) and an upper triangular matrix R.\n",
    "- *Eigenvalue Decomposition (EVD)*: Decomposes a matrix into eigenvectors and eigenvalues.\n",
    "- S*ingular Value Decomposition (SVD)*: Breaks down a matrix into three matrices: U, Σ (diagonal matrix), and Vᵀ.\n",
    "\n",
    "Here in this notebook we look into EVD and SVD of Matrix decomposition techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Value Decomposition (EVD)\n",
    "Eigen Value Decomposition(EVD) is similar to that of Diagonalization.\n",
    "- EVD is a factorization technique which expresses a square matrix **A** in terms of its eigenvalues and eigenvectors.\n",
    "- For a matrix **A** , EVD represnts or factorizes as **A = VΛV<sup>-1</sup>**.\n",
    "- **V** contains teh eigenvectors of **A**.\n",
    "- **Λ** is the diagonal matrix with eigenvalues of **A**.\n",
    "- EVD is applicable to any square matrix, whether or not it is symmetric.\n",
    "- **EVD is used in techniques like Principal Component Analysis (PCA) for dimensionality reduction and solving linear systems of equations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVD matrix:\n",
      "[[ 2.00000000e+00  2.33636299e-16]\n",
      " [ 2.26818301e-17 -1.00000000e+00]]\n",
      "A_evd natrix:\n",
      "[[ 4.  2.]\n",
      " [-5. -3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 2], [-5, -3]]) \n",
    "\n",
    "# Find eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "V = eigenvectors # eigenvectors of A.\n",
    "\n",
    "# Create diagonal matrix D having eigenvalues of A as diagonal elements\n",
    "Lambda = np.diag(eigenvalues)\n",
    "\n",
    "# Find the inverse of eigenvector matrix V\n",
    "Vinv = np.linalg.inv(V)\n",
    "\n",
    "# Perfrom EVD to the matrix A. evd_matrix(Λ) = V_inv*A*V\n",
    "evd_matrix = np.dot(np.dot(Vinv, A), V)\n",
    "print(\"EVD matrix:\")\n",
    "print(evd_matrix)\n",
    "\n",
    "# confirm that A is corresponding to evd_matrix. A = V*Λ*V_inv.\n",
    "A_evd = np.dot(V, np.dot(Lambda, Vinv))\n",
    "print(\"A_evd natrix:\")\n",
    "print(A_evd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonalization\n",
    "\n",
    "What is Diagonalization any way.  \n",
    "Firstly lets see what is a Diagonal matrix is?  \n",
    "\n",
    "**Diagonal Matrix** : It is a square matrix **D** which has its entire non-diagonal elements as zeroes.\n",
    "- The diagonal matrix only has diagonal elements.\n",
    "- They allow fast computation of determinants, powers, and inverses of a matrix.\n",
    "- The determinant of a diagonal matrix is the product of all its diagonal elements.\n",
    "- A matrix **D<sup>k</sup>** is given by all of its diagonal elements raised to the power of k.\n",
    "- The inverse of a diagonal matrix is given by **D<sup>-1</sup>** is the reciprocal of all of its elements if all of them are non-zero.\n",
    "\n",
    "Here we will look into how to tranform a matrix **A** into diagonal form for computational efficiency.\n",
    "\n",
    "**Diagonalization** :  \n",
    "- Matrix **A** is diagonalizable if it can be represented in the form of **A = PDP<sup>-1</sup>**.\n",
    "- **P** is a matrix whose columns are linearly independent eigenvectors of **A**.\n",
    "- **D** is a diagonal matrix with eigenvalues of **A** on its diagonal.\n",
    "- Diagonalization is possible only when **A** has a complete set of linearly independent eigenvectors.\n",
    "- Diagonalization is a specific case of EVD for diagonalizable matrices.\n",
    "- Symmetric matrices are always diagonalizable, while non-symmetric matrices may or may not be diagonalizable.\n",
    "- **Diagonalization is used for understanding linear transformations, solving systems of equations.**\n",
    "\n",
    "Both EVD and Diagonalization has same way of implementing but the difference comes when the matrix is about diagonalizable or not, Symmetric or not and the eigenvectors of the matrix are linearly independent or not. So considering these conditions we can perform appropriate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonalized Matrix:\n",
      "[[ 1.38196601e+01 -3.84003946e-16]\n",
      " [ 4.77991462e-15  3.61803399e+01]]\n",
      "A_diag:\n",
      "[[20. 10.]\n",
      " [10. 30.]]\n"
     ]
    }
   ],
   "source": [
    "# Define the matrix A (2x2 example)\n",
    "A = np.array([[20, 10],\n",
    "              [10, 30]])\n",
    "\n",
    "# Find eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "P = eigenvectors # Linearly independent eigenvectors of A.\n",
    "\n",
    "# Create diagonal matrix D having eigenvalues of A as diagonal elements\n",
    "D = np.diag(eigenvalues)\n",
    "\n",
    "# Find the inverse of eigenvector matrix P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Diagonalize the matrix : D = P_inv*A*P \n",
    "diagonalized_matrix = np.dot(np.dot(P_inv, A), P)\n",
    "\n",
    "print(\"Diagonalized Matrix:\")\n",
    "print(diagonalized_matrix)\n",
    "\n",
    "# A = P*D*P_inv  confirm that the diagonalized form of matrix A is D(diagonalized_matrix).\n",
    "A_diag = np.dot(np.dot(P, diagonalized_matrix), P_inv)\n",
    "print(\"A_diag:\")\n",
    "print(A_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "SVD is the central matrix decompostion method in linear algebra. \n",
    "- SVD factorizes the matrix **A** into three matrices **U, Σ, V<sup>T</sup>**.\n",
    "- For a matrix A of size **m**x**n** the vectors **U, Σ, V** are:\n",
    "- **U**: An **m**x**m** matrix containing orthonormal eigenvectors of **AA<sup>T</sup>**.\n",
    "- **Σ**: A diagonal matrix with singular values (positive square roots of eigenvalues of **AA<sup>T</sup>** or **A<sup>T</sup>A**).\n",
    "- **Vᵀ**: The transpose of an **m**x**m** matrix containing orthonormal eigenvectors of **A<sup>T</sup>A**.\n",
    "- **SVD has various applications like Dimensionality Reduction, Pseudo-inverse, Noise Reduciton and Image compression etc.**\n",
    "\n",
    "**A = UΣV<sup>T</sup>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[4 0]\n",
      " [0 3]]\n",
      "\n",
      "U matrix:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "Sigma matrix (as a diagonal matrix):\n",
      "[[4. 0.]\n",
      " [0. 3.]]\n",
      "\n",
      "Vt matrix:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "A_svd matrix:\n",
      "[[4. 0.]\n",
      " [0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# matrix A\n",
    "A = np.array([[4, 0],\n",
    "              [0, 3]])\n",
    "\n",
    "# Compute the SVD numpy has the function np.linalg.svd() function to get svd matrices.\n",
    "U, Sigma, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nU matrix:\")\n",
    "print(U)\n",
    "print(\"\\nSigma matrix (as a diagonal matrix):\")\n",
    "print(np.diag(Sigma))\n",
    "print(\"\\nVt matrix:\")\n",
    "print(Vt)\n",
    "\n",
    "# confirm that A = UΣV_t\n",
    "A_svd = np.dot(np.dot(U, np.diag(Sigma)), Vt)\n",
    "print(\"\\nA_svd matrix:\")\n",
    "print(A_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "Tensor is a mathematical object in an m-dimensional space of rank of n which has n indices, m<sup>n</sup> components while obeying certain transformation rules.\n",
    "\n",
    "In short a 0-dimensional Tensor is a scalar, 1-dimensional Tensor is a vector and 2-dimensional Tensor is a matrix and so on.,\n",
    "- The most important point to remember is that Tensors are dymnamic. Thats what it makes differ from a matrix.\n",
    "- Tensors will transform when interacting with other mathematical entities, matrices don't always have this property.\n",
    "- Refer to this wonderful article from **Steven Steinke** about fundamental differnece between <a href = 'https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c'>matrices and Tensor</a>.\n",
    "- We can create tensors from both tensorflow and PyTorch libraries of python. Official documentation will be helpful to you to understand it better. No difference will you find if yu already know about linear algebra, matrices.\n",
    "- <a href='https://www.tensorflow.org/guide/tensor'>Tensorflow tensors</a> , <a href='https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html'>PyTorch tensors</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will point out what are the uses of Tensors and why they have choosen over matrices in DeepLearning networks.\n",
    "- Tensors can play an important role in ML by encoding multi-dimensional data. \n",
    "- Can develop more robust models in DeepLearning than using matrices.\n",
    "- DeepLearning involves in computing hundreds of thousands of dimensions and fields. Tensors can store data in any no of N dimensoins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The topics covered in Linear Algebra are crucial for machine learning applications. More advanced topics are omitted here, as they serve specific purposes for particular algorithms or techniques. Learning algorithms in machine learning may involve selecting specific advanced topics. The discussed applications include data representation, manipulation, dimensionality reduction, solving linear equations, and image compression, highlighting the significance of Linear Algebra in machine learning.\n",
    "\n",
    "To delve deeper into Linear Algebra for machine learning, consider exploring topics such as Pseudo-Inverse calculation and applications like Dimensionality Reduction Techniques. The aforementioned topics provide a comprehensive understanding of Linear Algebra's role in machine learning. For additional learning resources, refer to the readme file.\n",
    "\n",
    "As of now stated there are many applications of Linear Algebra in machine learning, simply it is the heart of Machine Learning. Those applications drive the foremost importance of learning Linear Algebra while diving into Machine Learning. Further topics we will cover are probability, calculus, and statistics have almost equal significance in machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
